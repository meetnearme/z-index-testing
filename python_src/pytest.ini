[pytest]
# Add options for running pytest
addopts =
    --benchmark-autosave
    --benchmark-group-by=func
    --benchmark-columns=min,max,mean,stddev,median,iqr,outliers,rounds,iterations
    --benchmark-sort=mean
    --benchmark-histogram
    --benchmark-warmup=auto
    --benchmark-warmup-iterations=1
    --benchmark-min-rounds=5
    --benchmark-min-time=0.000005
    --benchmark-max-time=1.0
    --benchmark-calibration-precision=10
    --benchmark-disable-gc
    -v
    -s

# Specify the directories containing benchmarking files
testpaths =
    benchmarks

# Specify the test file patterns for benchmarking
python_files =
    benchmark_*.py

# Specify the test function name patterns for benchmarking
python_functions =
    benchmark_*

# Configure benchmark storage and output
benchmark_storage = .benchmarks
benchmark_save = True
benchmark_autosave = True
benchmark_save_data = True
benchmark_compare = "*"
benchmark_compare_fail = min:5%
benchmark_histogram_bins = 20
benchmark_round_digits = 6

# Specify additional plugins
filterwarnings =
    ignore::pytest.PytestCacheWarning

# Configure logging level
log_level = INFO

# Configure JUnit XML report generation
junit_family = xunit2

# Configure coverage settings
[coverage:run]
branch = True
source =
    src

[coverage:report]
exclude_lines =
    pragma: no cover
    def __repr__
    if self.debug:
    if settings.DEBUG
    raise AssertionError
    raise NotImplementedError
    if 0:
    if __name__ == .__main__.:
